{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "결측치 처리법 3가지\n",
    "1. 결측행 제거\n",
    "2. 결측값 대체\n",
    "3. 결측값 대체 및 대체유무에 대해 기입\n",
    "\n",
    "2번과 3번에서 \"대체\"시에 SimpleImputer를 활용\n",
    "Kaggle: SimpleImputer는 평균값으로 값을 대체. 사람들은 보통 결측값을 complex한 방법으로 대체하기 위해\n",
    "다양한 방법을 활용 (ex. regression imputation). 그러나 이는 복잡한 ML모델이 활용된다면,\n",
    "이에 대한 이점은 거의 없어진다고 보면 된다.\n",
    "\n",
    "==============================================================================================================================\n",
    "\n",
    "궁금: fit_transform과 transform의 차이는 무엇인가?\n",
    "\n",
    "fit_transform() 메소드는 말 그대로 fit()한 다음에 transform() 하는 것입니다. \n",
    "여기서 fit()이란 정규화 즉, 통계에서 정규분포를 만들게 하기 위해서 𝑥 값에서 평균을 빼고 \n",
    "그 값을 다시 표준편차로 나누어주는 작업을 하는데 \n",
    "이 작업을 하기 위해 평균 𝜇과  표준편차 𝜎를 계산하는 작업이 fit() 이고, transform()은 정규화 작업을 해주는 것입니다. \n",
    "(𝑥-𝜇)/𝜎 ==> 새로운 𝑥′ 생기는 것이죠\n",
    "\n",
    "트레이닝 데이터에 대해서 fit 작업과 transform 작업을 적용해주는 것이 fit_transform이고 \n",
    "여기서 계산된 평균 𝜇과  표준편차 𝜎를 동일하게 테스트 데이터에 적용해서 정규화 작업을 해주는 경우는 transform()만 적용합니다.  \n",
    "테스트 세트에도 트레이닝 데이터에 적용되었던 동일한 평균과 표준편차를 적용하기 위해서 \n",
    "이전에 fit_transform()에서 계산된 값들이 저장된 상태에서 transform()에 적용되는 것입니다.\n",
    "아래 출처를 따라가 보시면 fit_transform의 파라미터들을 자세히 살펴보실 수 있습니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 열에서 결측치가 있는 행을 제거할 때 subset 인자를 활용\n",
    "X_full.dropna(axis=0, subset=['LotFrontage'], inplace=True)\n",
    "\n",
    "# 자료형이 object가 아닌 것만 불러오기\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "# SimpleImputer에 평균 대신 중간값으로 대체할 수 있는 방법\n",
    "final_imputer = SimpleImputer(strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "열 별 결측치의 개수를 보고 해당 열을 제거해야하는가? 생각\n",
    "--> 열 자체를 제거하는 것은 귀중한 데이터를 제거하는 것이다. 하지만, 결측치의 개수가 열 전체의 개수의 20%미만이라면 제거하지말자.\n",
    "오히려 이럴땐 대체하는 것이 바람직 할 수 있다.\n",
    "\n",
    "====================================================================================================================\n",
    "\n",
    "데이터 세트에 결측값이 너무 적기 때문에 열을 완전히 삭제하는 것보다 귀책성이 더 잘 수행될 것으로 예상한다. \n",
    "그러나 열을 떨어뜨리는 것이 약간 더 낫다는 것을 알 수 있습니다. \n",
    "이는 데이터 세트의 노이즈에 부분적으로 기인할 수 있지만, \n",
    "또 다른 잠재적인 설명은 귀책 방법이 이 데이터 세트와 크게 일치하지 않는다는 것이다.\n",
    "\n",
    "즉, 평균 값을 채우는 대신 모든 결측값을 0 값으로 설정하거나, \n",
    "가장 자주 발생하는 값을 채우거나, 다른 방법을 사용하는 것이 더 합리적일 수 있습니다.\n",
    "\n",
    "각 열을 따라 중위수 값을 채우는 것이 더 의미가 있습니까? \n",
    "아니면 각 열을 따라 최소값을 채워 더 나은 결과를 얻을 수 있을까요? \n",
    "이 경우에 무엇이 최선인지 확실하지 않지만, 아마도 우리는 즉시 몇 가지 옵션을 배제할 수 있습니다. \n",
    "    예를 들어, 이 열의 결측값을 0으로 설정하면 끔찍한 결과를 초래할 수 있습니다!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "카테고리 변수 CITY처럼 경기, 강원을 value로 갖는 변수의 경우 처리방법\n",
    "1. Drop\n",
    "2. 숫자에 mapping\n",
    " - sklearn의 labelencoder를 쓰면 편리 --> 그러나, 각 값에 무작위 숫자를 대응\n",
    " - 이렇게 되면 크기요소를 반영하기 힘들듯 싶은데?\n",
    "3. one-hot-encoding\n",
    " - sklearn의 OneHotEncoder를 쓴다.\n",
    " - 파라미터 설명\n",
    "  -- handle_unknown='ignore': train에 원-핫-인코딩 후, validation data에서 없는 카테고리 발견시 오류발생x \n",
    "  -- sparse=False: 반환객체가 numpy\n",
    "  \n",
    "*** labelencoder나 onehotencoder 활용 전에 꼭 train과 valid data의 구성이 맞는지 확인해!\n",
    "*** onehotencoding은 변수의 value수가 적을때 많이 활용되고, nominal이지만 value의 수가 많다면 아싸리 제거하거나 labelencoding\n",
    "'''\n",
    "\n",
    "궁금! : handle_unknown='ignore'을 실시시, 없는 값은 그냥 제외되는 것인지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map과 lambda함수\n",
    "\n",
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols)) #map과 lambda는 항상 같이 다님. map(lambda 인자: 함수식, 넣을 인자)\n",
    "d = dict(zip(object_cols, object_nunique)) # [(, ), (, ), (, ) ]등의 형태로 다수 들어가며, dict함수를 거침으로써 dictionary로 변환\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 구성법1\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() < 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 구성법2\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터가 적을때, CV를 쓰자. 데이터가 적은데, Validation set까지 빼내려먼 훈련data가 적어지기 떄문.\n",
    "데이터가 크면, single validation을 실시하자! (크면 시간이 많이 걸리잖아!)\n",
    "Pipline을 활용하면 CV가 훨씬 쉬움!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                              ('model', RandomForestRegressor(n_estimators=50,\n",
    "                                                              random_state=0))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average MAE score (across experiments):\")\n",
    "print(scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
